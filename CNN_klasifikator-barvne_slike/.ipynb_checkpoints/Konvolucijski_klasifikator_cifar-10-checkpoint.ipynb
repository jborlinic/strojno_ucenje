{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konvolucijski klasifikator\n",
    "Ta primer temelji na [CNN konvolucijski mreži](https://github.com/jborlinic/machine_learning/blob/master/CNN_konvolucijske_mre%C5%BEe/CNN_s_Tensorflow.ipynb), le da ta model poizkuša klasificirati nabor podatkov [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html).  \n",
    "\n",
    "Je zelo dober primer, kako začetno treniranje pri težjih modelih ne pomeni, da je model slab. Treniral sem 9 podobnih modelov, vsakega za približbno 30 min (2000 iteracij) in so vsi stagnirali med 10-17% natančnostjo na učnem naboru podatkov.  \n",
    "Kasneje sem, kot zadnji poizkus, pognal progam za 40000 iteracij (~2h 40min) in model se je začel učiti komaj po 3900 korakih. Na koncu je dosegel natančnost 88% na učnih podatkih."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = '../.datasets/cifar-10/'\n",
    "TRAIN_IMAGES_FILE = '%strain_images.csv' %DATA_DIR\n",
    "TRAIN_LABELS_FILE = '%strain_labels.csv' %DATA_DIR\n",
    "TEST_IMAGES_FILE = '%stest_images.csv' %DATA_DIR\n",
    "TEST_LABELS_FILE = '%stest_labels.csv' %DATA_DIR\n",
    "\n",
    "LOG_DIR = 'log/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(input, \n",
    "               size_in, \n",
    "               size_out, \n",
    "               filter_size=5, \n",
    "               strides=1, \n",
    "               name=\"conv\"):\n",
    "    \"\"\"\n",
    "    A basic 3d convolution layer, with args:\n",
    "    input - a 5-D tensor of shape \n",
    "        [#of_images, #image_width, #image_height, #image_depth, #of_features]\n",
    "        example tensor shape: [-1, 32, 32, 3, 1]\n",
    "    filer_size - the size of the convolution filter\n",
    "    strides - the size of the strides to take\n",
    "    size_in - # of layers per image of input\n",
    "    size_out - # of layers per image of output\n",
    "    name - the name of the layer\n",
    "\n",
    "    the function returns the RELU activation layer of the convolution:\n",
    "    a 5-D tensor of shape [#of_images, #image_width, #image_height, #image_depth, #of_features]\n",
    "    where #of_layers is equal to the arg size_out\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([filter_size, filter_size, 3, size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"b\")\n",
    "        conv = tf.nn.conv3d(input, w, strides=[1, strides, strides, strides, 1], padding=\"SAME\")\n",
    "        act = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activation\", act)\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(input, kernel, strides, name=\"max_pool\"):\n",
    "    \"\"\"\n",
    "    A 3d max_pool layer with args:\n",
    "    input - a 5-D tensor of shape \n",
    "        [#of_images, #image_width, #image_height, #image_depth, #of_features]\n",
    "        example tensor shape [-1, 32, 32, 3, 50]\n",
    "    kernel - a 5-D tensor of shape \n",
    "        [1, ksize1, ksize2, ksize3, 1]\n",
    "        example tensor shape [1, 2, 2, 3, 1]\n",
    "    strides - a 5-D tensor of shape\n",
    "        [1, strides1, strides2, 1, 1]\n",
    "    name - the name of the layer\n",
    "\n",
    "    this function outputs a 5-D tensor after the max_pool_3D operation  \n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        max_pool_3d = tf.nn.max_pool3d(input, ksize=kernel, strides=strides, padding=\"SAME\")\n",
    "        tf.summary.histogram(\"max_pooling\", max_pool_3d)\n",
    "        return max_pool_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_layer(input, size_in, size_out, name=\"fully_connected_layer\"):\n",
    "    \"\"\"\n",
    "    a fully connected layer with args:\n",
    "    input - a 2-D tensor of shape [#of_images, #of_features]\n",
    "    size_in - the number of features\n",
    "    size_out - the number of features of the output\n",
    "    name - the name of the layer\n",
    "\n",
    "    this function returns a 2-D tensor of shape [#of_images, #of_features_out]\n",
    "    where #of_features_out equals size_out\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"b\")\n",
    "        act = tf.nn.relu(tf.matmul(input, w) + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_layer(input, keep_probability, name=\"dropout\"):\n",
    "    \"\"\"\n",
    "    a dropout layer, that performs a simple dropout operation with args:\n",
    "    input - a 2-D tensor of shape [#of_images, #of_features]\n",
    "    keep_probability - a scalar the same type as the input, determins the probability of keeping each value\n",
    "    name - the name of the layer\n",
    "\n",
    "    outputs a 2-D tensor of the same shape as the input\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        do = tf.nn.dropout(input, keep_probability)\n",
    "        tf.summary.histogram(\"dropout\", do)\n",
    "        return do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(dataset, labelset, batch_size=100):\n",
    "    \"\"\"\n",
    "    a function that returns a randomized batch of data/labels from the whole dataset/labelset.\n",
    "    arg:\n",
    "    dataset - a pd dataframe of all data\n",
    "    labelset - a pd dataframe of all labels\n",
    "        // dataset on index i should correspond to the labelset index i\n",
    "    batch_size - the size of the needed random batch\n",
    "\n",
    "    returns a pair [data, labels] of data and labes of size batch_size\n",
    "    \"\"\"\n",
    "    selector = np.concatenate((np.ones(batch_size),\n",
    "                              np.zeros(dataset.shape[0] - batch_size)), axis = 0)\n",
    "\n",
    "    np.random.shuffle(selector)\n",
    "    selector = selector.astype('bool')\n",
    "\n",
    "    \n",
    "    batch_output = []\n",
    "    \n",
    "    batch_output.append(dataset.iloc[selector])\n",
    "    batch_output.append(labelset.iloc[selector])\n",
    "    \n",
    "    return batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_output(output):\n",
    "    df = pd.DataFrame(output)\n",
    "    df.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cifar_10_model(dataset, labelset, learning_rate, numberOfSteps=5001):\n",
    "    tf.reset_default_graph()\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 3072], name='x')\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10], name='labels')\n",
    "    \n",
    "    #lets normalize our image vector\n",
    "    x0 = tf.multiply(x, 1/255, name=\"normalizer\")\n",
    "\n",
    "    \"\"\"\n",
    "    We have to transform the 2-D tensor to a 5-D tensor for the 3D convolution, where:\n",
    "     - 1-dim is the number of example images\n",
    "     - 2-dim is the width of the image (32)\n",
    "     - 3-dim is the height of the image(32)\n",
    "     - 4-dim is the depth of the image (3, RGB)\n",
    "     - 5-dim is the number of features, ATM = 1\n",
    "\n",
    "    The input image is stored as RRRR...RGGGG...GBBBB...B: \n",
    "     - the first 1024 columns are 32x32 red values\n",
    "     - the second 1024 columns are 32x32 green values\n",
    "     - the third 1024 columns are 32x32 blue values\n",
    "     \n",
    "    Each line has to be transformed in the following way: \n",
    "        1 x 3072 > 1 x 3 x 1024 > 1 x 1024 x 3 > 1 x 32 x 32 x 3 > 1 x 32 x 32 x 3 x 1\n",
    "    \"\"\"\n",
    "    \n",
    "    x1 = tf.reshape(x0, [-1,3,1024])\n",
    "    x2 = tf.transpose(x1, perm=[0,2,1])\n",
    "    x_image_out = tf.reshape(x2, [-1,32,32,3])\n",
    "    \n",
    "    tf.summary.image(\"input\", x_image_out, 10)\n",
    "\n",
    "    x_image = tf.reshape(x2, [-1,32,32,3,1])\n",
    "    \n",
    "    \"\"\"\n",
    "    Time to define our model. \n",
    "    It will consist of 2 conv layers 2 max pool layers, 2 fc layers and 1 dropout layer\n",
    "    sizes will vary in the following way:\n",
    "    [-1, 32, 32, 3, 1]\n",
    "    conv1\n",
    "    [-1, 32, 32, 3, 50]\n",
    "    max_pool\n",
    "    [-1, 16, 16, 1, 50]\n",
    "    conv2\n",
    "    [-1, 16, 16, 1, 100]\n",
    "    max_pool\n",
    "    [-1, 8, 8, 1, 100]\n",
    "    flatten\n",
    "    [-1, 8*8*100 = 6400]\n",
    "    fc_layer\n",
    "    [-1, 1000]\n",
    "    drouput\n",
    "    [-1, 1000]\n",
    "    fc_layer\n",
    "    [-1, 10]\n",
    "    \"\"\"\n",
    "    \n",
    "    conv1 = conv_layer(x_image, 1, 32, filter_size=5, strides=1, name=\"conv_layer_1\")\n",
    "    \n",
    "    mp1 = max_pool(conv1, [1,2,2,1,1], [1,2,2,1,1], name=\"max_pool_1\")\n",
    "    \n",
    "    conv2 = conv_layer(mp1, 32, 32, filter_size=5, strides=1, name=\"conv_layer_2\")\n",
    "    \n",
    "    mp2 = max_pool(conv2, [1,2,2,1,1], [1,2,2,1,1], name=\"max_pool_2\")\n",
    "\n",
    "    conv3 = conv_layer(mp2, 32, 64, filter_size=5, strides=1, name=\"conv_layer_3\")\n",
    "    \n",
    "    flat = tf.reshape(conv3, [-1, 8*8*3*64]) # = 12288\n",
    "   \n",
    "    tf.summary.histogram(\"flattened tensor\", flat)\n",
    "\n",
    "    fc1 = fc_layer(flat, 12288, 64, name=\"fc_layer_1\")\n",
    "    \n",
    "    drop = dropout_layer(fc1, 0.5, name=\"dropout\")\n",
    "    \n",
    "    logits = fc_layer(fc1, 64, 10, name=\"fc_layer_2\")\n",
    "\n",
    "    \n",
    "    # define cross entropy\n",
    "    with tf.name_scope('xent'):\n",
    "        xent = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y), name=\"x_ent\")\n",
    "        tf.summary.scalar('cross_entropy', xent)\n",
    "    \n",
    "    # define the trainig step using AdamOptimizer and a learning rate of 1E-4 \n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)\n",
    "\n",
    "    # define accuracy\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "  \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    writer = tf.summary.FileWriter(LOG_DIR)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    checkpointName = LOG_DIR + 'myCIFAR-10_model'\n",
    "    \n",
    "    print('Trying to load previous model from: %s' %(LOG_DIR))\n",
    "    try: \n",
    "        f = open(LOG_DIR + 'checkpoint', 'r')\n",
    "        cp_path = f.readline()\n",
    "        f.close()\n",
    "        cp_path = cp_path[cp_path.find('\"')+1 : cp_path.rfind('\"')]\n",
    "        cp_path = LOG_DIR + cp_path\n",
    "        saver.restore(sess, cp_path)\n",
    "        print('Model succesfully restored from: %s.' %(cp_path))\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print('Can not load model: no checkpoint found.')\n",
    "        \n",
    "        \n",
    "    for i in range(numberOfSteps):\n",
    "    \n",
    "        batch = get_batch(dataset, labelset, batch_size=128)\n",
    "\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y: batch[1]})\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            [s, train_acc, output] = sess.run([merged_summary, accuracy, logits], feed_dict={x: batch[0], y: batch[1]})\n",
    "            writer.add_summary(s, i)\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            #train_acc = sess.run(accuracy, feed_dict={x: batch[0], y: batch[1]})\n",
    "            print(\"Step %d, training accuracy %g\" %(i, train_acc))\n",
    "            save_output(output)\n",
    "\n",
    "        if i % 1000 == 0 and i > 0:\n",
    "            print('Saving checkpoint.')\n",
    "            saver.save(sess, checkpointName, global_step=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name flattened tensor is illegal; using flattened_tensor instead.\n",
      "Trying to load previous model from: log/\n",
      "INFO:tensorflow:Restoring parameters from log/myCIFAR-10_model-2000\n",
      "Model succesfully restored from: log/myCIFAR-10_model-2000.\n",
      "Step 0, training accuracy 0.851562\n",
      "Step 500, training accuracy 0.835938\n",
      "Step 1000, training accuracy 0.835938\n",
      "Saving checkpoint.\n",
      "Step 1500, training accuracy 0.859375\n",
      "Step 2000, training accuracy 0.835938\n",
      "Saving checkpoint.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset = pd.DataFrame.from_csv(TRAIN_IMAGES_FILE, header=None, index_col=None)\n",
    "    labelset = pd.DataFrame.from_csv(TRAIN_LABELS_FILE, header=None, index_col=None)\n",
    "\n",
    "    learning_rate = 1E-4\n",
    "    cifar_10_model(dataset, labelset, learning_rate, numberOfSteps=2001)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
