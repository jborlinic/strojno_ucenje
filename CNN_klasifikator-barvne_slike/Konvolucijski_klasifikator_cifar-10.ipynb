{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konvolucijski klasifikator\n",
    "Ta primer temelji na [CNN konvolucijski mreži](https://github.com/jborlinic/machine_learning/blob/master/CNN_konvolucijske_mre%C5%BEe/CNN_s_Tensorflow.ipynb), le da ta model poizkuša klasificirati nabor podatkov [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html).  \n",
    "\n",
    "Je zelo dober primer, kako začetno treniranje pri težjih modelih ne pomeni, da je model slab. Treniral sem 9 podobnih modelov, vsakega za približbno 30 min (2000 iteracij) in so vsi stagnirali med 10-17% natančnostjo na učnem naboru podatkov.  \n",
    "Kasneje sem, kot zadnji poizkus, pognal progam za 40000 iteracij (~2h 40min) in model se je začel učiti komaj po 3900 korakih. Na koncu je dosegel natančnost 88% na učnih podatkih."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = '../.datasets/cifar-10/'\n",
    "TRAIN_IMAGES_FILE = '%strain_images.csv' %DATA_DIR\n",
    "TRAIN_LABELS_FILE = '%strain_labels.csv' %DATA_DIR\n",
    "TEST_IMAGES_FILE = '%stest_images.csv' %DATA_DIR\n",
    "TEST_LABELS_FILE = '%stest_labels.csv' %DATA_DIR\n",
    "\n",
    "LOG_DIR = 'log/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(input, \n",
    "               size_in, \n",
    "               size_out, \n",
    "               filter_size=5, \n",
    "               strides=1, \n",
    "               name=\"conv\"):\n",
    "    \"\"\"\n",
    "    A basic 3d convolution layer, with args:\n",
    "    input - a 5-D tensor of shape \n",
    "        [#of_images, #image_width, #image_height, #image_depth, #of_features]\n",
    "        example tensor shape: [-1, 32, 32, 3, 1]\n",
    "    filer_size - the size of the convolution filter\n",
    "    strides - the size of the strides to take\n",
    "    size_in - # of layers per image of input\n",
    "    size_out - # of layers per image of output\n",
    "    name - the name of the layer\n",
    "\n",
    "    the function returns the RELU activation layer of the convolution:\n",
    "    a 5-D tensor of shape [#of_images, #image_width, #image_height, #image_depth, #of_features]\n",
    "    where #of_layers is equal to the arg size_out\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([filter_size, filter_size, 3, size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"b\")\n",
    "        conv = tf.nn.conv3d(input, w, strides=[1, strides, strides, strides, 1], padding=\"SAME\")\n",
    "        act = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activation\", act)\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(input, kernel, strides, name=\"max_pool\"):\n",
    "    \"\"\"\n",
    "    A 3d max_pool layer with args:\n",
    "    input - a 5-D tensor of shape \n",
    "        [#of_images, #image_width, #image_height, #image_depth, #of_features]\n",
    "        example tensor shape [-1, 32, 32, 3, 50]\n",
    "    kernel - a 5-D tensor of shape \n",
    "        [1, ksize1, ksize2, ksize3, 1]\n",
    "        example tensor shape [1, 2, 2, 3, 1]\n",
    "    strides - a 5-D tensor of shape\n",
    "        [1, strides1, strides2, 1, 1]\n",
    "    name - the name of the layer\n",
    "\n",
    "    this function outputs a 5-D tensor after the max_pool_3D operation  \n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        max_pool_3d = tf.nn.max_pool3d(input, ksize=kernel, strides=strides, padding=\"SAME\")\n",
    "        tf.summary.histogram(\"max_pooling\", max_pool_3d)\n",
    "        return max_pool_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_layer(input, size_in, size_out, name=\"fully_connected_layer\"):\n",
    "    \"\"\"\n",
    "    a fully connected layer with args:\n",
    "    input - a 2-D tensor of shape [#of_images, #of_features]\n",
    "    size_in - the number of features\n",
    "    size_out - the number of features of the output\n",
    "    name - the name of the layer\n",
    "\n",
    "    this function returns a 2-D tensor of shape [#of_images, #of_features_out]\n",
    "    where #of_features_out equals size_out\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"b\")\n",
    "        act = tf.nn.relu(tf.matmul(input, w) + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_layer(input, keep_probability, name=\"dropout\"):\n",
    "    \"\"\"\n",
    "    a dropout layer, that performs a simple dropout operation with args:\n",
    "    input - a 2-D tensor of shape [#of_images, #of_features]\n",
    "    keep_probability - a scalar the same type as the input, determins the probability of keeping each value\n",
    "    name - the name of the layer\n",
    "\n",
    "    outputs a 2-D tensor of the same shape as the input\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        do = tf.nn.dropout(input, keep_probability)\n",
    "        tf.summary.histogram(\"dropout\", do)\n",
    "        return do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(dataset, labelset, batch_size=100):\n",
    "    \"\"\"\n",
    "    a function that returns a randomized batch of data/labels from the whole dataset/labelset.\n",
    "    arg:\n",
    "    dataset - a pd dataframe of all data\n",
    "    labelset - a pd dataframe of all labels\n",
    "        // dataset on index i should correspond to the labelset index i\n",
    "    batch_size - the size of the needed random batch\n",
    "\n",
    "    returns a pair [data, labels] of data and labes of size batch_size\n",
    "    \"\"\"\n",
    "    selector = np.concatenate((np.ones(batch_size),\n",
    "                              np.zeros(dataset.shape[0] - batch_size)), axis = 0)\n",
    "\n",
    "    np.random.shuffle(selector)\n",
    "    selector = selector.astype('bool')\n",
    "\n",
    "    \n",
    "    batch_output = []\n",
    "    \n",
    "    batch_output.append(dataset.iloc[selector])\n",
    "    batch_output.append(labelset.iloc[selector])\n",
    "    \n",
    "    return batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_output(output):\n",
    "    df = pd.DataFrame(output)\n",
    "    df.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cifar_10_model(dataset, labelset, learning_rate, numberOfSteps=5001):\n",
    "    tf.reset_default_graph()\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 3072], name='x')\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10], name='labels')\n",
    "    \n",
    "    #lets normalize our image vector\n",
    "    x0 = tf.multiply(x, 1/255, name=\"normalizer\")\n",
    "\n",
    "    \"\"\"\n",
    "    We have to transform the 2-D tensor to a 5-D tensor for the 3D convolution, where:\n",
    "     - 1-dim is the number of example images\n",
    "     - 2-dim is the width of the image (32)\n",
    "     - 3-dim is the height of the image(32)\n",
    "     - 4-dim is the depth of the image (3, RGB)\n",
    "     - 5-dim is the number of features, ATM = 1\n",
    "\n",
    "    The input image is stored as RRRR...RGGGG...GBBBB...B: \n",
    "     - the first 1024 columns are 32x32 red values\n",
    "     - the second 1024 columns are 32x32 green values\n",
    "     - the third 1024 columns are 32x32 blue values\n",
    "     \n",
    "    Each line has to be transformed in the following way: \n",
    "        1 x 3072 > 1 x 3 x 1024 > 1 x 1024 x 3 > 1 x 32 x 32 x 3 > 1 x 32 x 32 x 3 x 1\n",
    "    \"\"\"\n",
    "    \n",
    "    x1 = tf.reshape(x0, [-1,3,1024])\n",
    "    x2 = tf.transpose(x1, perm=[0,2,1])\n",
    "    x_image_out = tf.reshape(x2, [-1,32,32,3])\n",
    "    \n",
    "    tf.summary.image(\"input\", x_image_out, 10)\n",
    "\n",
    "    x_image = tf.reshape(x2, [-1,32,32,3,1])\n",
    "    \n",
    "    \"\"\"\n",
    "    Time to define our model. \n",
    "    It will consist of 2 conv layers 2 max pool layers, 2 fc layers and 1 dropout layer\n",
    "    sizes will vary in the following way:\n",
    "    [-1, 32, 32, 3, 1]\n",
    "    conv1\n",
    "    [-1, 32, 32, 3, 50]\n",
    "    max_pool\n",
    "    [-1, 16, 16, 1, 50]\n",
    "    conv2\n",
    "    [-1, 16, 16, 1, 100]\n",
    "    max_pool\n",
    "    [-1, 8, 8, 1, 100]\n",
    "    flatten\n",
    "    [-1, 8*8*100 = 6400]\n",
    "    fc_layer\n",
    "    [-1, 1000]\n",
    "    drouput\n",
    "    [-1, 1000]\n",
    "    fc_layer\n",
    "    [-1, 10]\n",
    "    \"\"\"\n",
    "    \n",
    "    conv1 = conv_layer(x_image, 1, 32, filter_size=5, strides=1, name=\"conv_layer_1\")\n",
    "    \n",
    "    mp1 = max_pool(conv1, [1,2,2,1,1], [1,2,2,1,1], name=\"max_pool_1\")\n",
    "    \n",
    "    conv2 = conv_layer(mp1, 32, 32, filter_size=5, strides=1, name=\"conv_layer_2\")\n",
    "    \n",
    "    mp2 = max_pool(conv2, [1,2,2,1,1], [1,2,2,1,1], name=\"max_pool_2\")\n",
    "\n",
    "    conv3 = conv_layer(mp2, 32, 64, filter_size=5, strides=1, name=\"conv_layer_3\")\n",
    "    \n",
    "    flat = tf.reshape(conv3, [-1, 8*8*3*64]) # = 12288\n",
    "   \n",
    "    tf.summary.histogram(\"flattened tensor\", flat)\n",
    "\n",
    "    fc1 = fc_layer(flat, 12288, 64, name=\"fc_layer_1\")\n",
    "    \n",
    "    drop = dropout_layer(fc1, 0.5, name=\"dropout\")\n",
    "    \n",
    "    logits = fc_layer(fc1, 64, 10, name=\"fc_layer_2\")\n",
    "\n",
    "    \n",
    "    # define cross entropy\n",
    "    with tf.name_scope('xent'):\n",
    "        xent = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y), name=\"x_ent\")\n",
    "        tf.summary.scalar('cross_entropy', xent)\n",
    "    \n",
    "    # define the trainig step using AdamOptimizer and a learning rate of 1E-4 \n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)\n",
    "\n",
    "    # define accuracy\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "  \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    writer = tf.summary.FileWriter(LOG_DIR)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    checkpointName = LOG_DIR + 'myCIFAR-10_model'\n",
    "    \n",
    "    print('Trying to load previous model from: %s' %(LOG_DIR))\n",
    "    try: \n",
    "        f = open(LOG_DIR + 'checkpoint', 'r')\n",
    "        cp_path = f.readline()\n",
    "        f.close()\n",
    "        cp_path = cp_path[cp_path.find('\"')+1 : cp_path.rfind('\"')]\n",
    "        cp_path = LOG_DIR + cp_path\n",
    "        saver.restore(sess, cp_path)\n",
    "        print('Model succesfully restored from: %s.' %(cp_path))\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print('Can not load model: no checkpoint found.')\n",
    "        \n",
    "        \n",
    "    for i in range(numberOfSteps):\n",
    "    \n",
    "        batch = get_batch(dataset, labelset, batch_size=128)\n",
    "\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y: batch[1]})\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            [s, train_acc, output] = sess.run([merged_summary, accuracy, logits], feed_dict={x: batch[0], y: batch[1]})\n",
    "            writer.add_summary(s, i)\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            #train_acc = sess.run(accuracy, feed_dict={x: batch[0], y: batch[1]})\n",
    "            print(\"Step %d, training accuracy %g\" %(i, train_acc))\n",
    "            save_output(output)\n",
    "\n",
    "        if i % 1000 == 0 and i > 0:\n",
    "            print('Saving checkpoint.')\n",
    "            saver.save(sess, checkpointName, global_step=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name flattened tensor is illegal; using flattened_tensor instead.\n",
      "Trying to load previous model from: log/9/\n",
      "Can not load model: no checkpoint found.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-04e3d9e30537>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1E-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcifar_10_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumberOfSteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-45f1b26c3b31>\u001b[0m in \u001b[0;36mcifar_10_model\u001b[0;34m(dataset, labelset, learning_rate, numberOfSteps)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jborlinic/anaconda2/envs/jupyter-tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jborlinic/anaconda2/envs/jupyter-tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jborlinic/anaconda2/envs/jupyter-tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/jborlinic/anaconda2/envs/jupyter-tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jborlinic/anaconda2/envs/jupyter-tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset = pd.DataFrame.from_csv(TRAIN_IMAGES_FILE, header=None, index_col=None)\n",
    "    labelset = pd.DataFrame.from_csv(TRAIN_LABELS_FILE, header=None, index_col=None)\n",
    "\n",
    "    learning_rate = 1E-4\n",
    "    cifar_10_model(dataset, labelset, learning_rate, numberOfSteps=40001)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:jupyter-tf]",
   "language": "python",
   "name": "conda-env-jupyter-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
