{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trenutno __ne__ dela\n",
    "\n",
    "Uporabi ***ep_keras_rnn.py*** s pomoƒçjo ***main.py***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "### parameters\n",
    "\n",
    "NUM_STEPS = 128\n",
    "NUM_PRED = NUM_STEPS // 4 \n",
    "BATCH_SIZE = 5\n",
    "print(NUM_PRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A function that transforms raw data to an array of sequence data.\n",
    "# The attribute raw_data should be a numpy 2-D array of float32 numbers.\n",
    "# Each row represents a trade and has 2 columns, price (column 0) and amount (column 1)\n",
    "def transform_to_seq(raw_data):\n",
    "    # The output (y) is the prediction of bitcoin prices of the next few (#num_pred) trades \n",
    "    # For the training data, this is represented by the real next (num_pred) trade prices.\n",
    "    \n",
    "    # To get the most of our data, we remove the last #num_pred trade prices and use them as \n",
    "    # the labels of the last input x\n",
    "    \n",
    "    temp_y = raw_data[len(raw_data) - NUM_PRED:, 0]\n",
    "    raw_data = raw_data[:len(raw_data) - NUM_PRED]\n",
    "    \n",
    "\n",
    "    # Remove the excess data from the start of the timeseries, if\n",
    "    # the number of steps (num_steps) doesn't devide the size of the remaining data.\n",
    "    \n",
    "    if len(raw_data) % NUM_STEPS != 0:\n",
    "        raw_data = raw_data[len(raw_data)%NUM_STEPS:]\n",
    "        \n",
    "    # Split the whole data into chunks, each the length of number of steps (num_steps).\n",
    "    d = len(raw_data) // NUM_STEPS\n",
    "    x = np.array(np.split(raw_data, d))\n",
    "    \n",
    "    # Gather the rest of the labels from x and append the last label from before.\n",
    "    \n",
    "    y = x[1:,:NUM_PRED, 0]\n",
    "    y = np.insert(y, -1, temp_y, axis=0)\n",
    "    \n",
    "    # Finnaly we return the transformed sequential data as a dictionary, \n",
    "    # with input x and labels y\n",
    "    return {\"x\":x, \"y\": y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A function that returns a random sample from the dictionary data (x: , y: )\n",
    "# Each (x and y) should represent a numpy array, where the each entry in the 1st dim,\n",
    "# represents an input or output vector.\n",
    "def batcher(data):\n",
    "    # Create a random mask of indexes. Attributes: max index: length of data,\n",
    "    # number of indexes: batch_size, with no recurring indexes.\n",
    "    mask = np.random.choice(range(len(data['x'])), BATCH_SIZE, replace=False)\n",
    "    \n",
    "    # Create the x and y batch of the same indexes.\n",
    "    batch = {}\n",
    "    batch['x'] = data['x'][mask]\n",
    "    batch['y'] = data['y'][mask]\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': array([[[  1.05881000e+03,   6.37771000e-01],\n",
      "        [  1.05752000e+03,   1.82890000e+00],\n",
      "        [  1.05752000e+03,   1.53000000e-01],\n",
      "        ..., \n",
      "        [  1.05114000e+03,   3.40951510e-01],\n",
      "        [  1.05307000e+03,   5.82782500e-02],\n",
      "        [  1.05134000e+03,   1.30234000e+00]],\n",
      "\n",
      "       [[  1.05134000e+03,   8.29170000e-01],\n",
      "        [  1.05133000e+03,   9.90000000e-02],\n",
      "        [  1.05076000e+03,   1.65810000e-01],\n",
      "        ..., \n",
      "        [  1.05774000e+03,   2.44900000e-02],\n",
      "        [  1.05825000e+03,   2.40000000e-02],\n",
      "        [  1.05854000e+03,   3.88165000e-02]],\n",
      "\n",
      "       [[  1.06278000e+03,   1.00584950e-01],\n",
      "        [  1.06390000e+03,   1.05890700e-01],\n",
      "        [  1.06282000e+03,   3.00000000e-02],\n",
      "        ..., \n",
      "        [  1.06498000e+03,   1.00000000e+00],\n",
      "        [  1.06498000e+03,   6.10200000e-01],\n",
      "        [  1.06498000e+03,   2.68000000e-02]],\n",
      "\n",
      "       [[  1.05854000e+03,   4.21835000e-02],\n",
      "        [  1.05910000e+03,   2.01149930e-01],\n",
      "        [  1.05912000e+03,   2.34236000e-01],\n",
      "        ..., \n",
      "        [  1.06484000e+03,   9.36863750e-01],\n",
      "        [  1.06478000e+03,   1.90014462e+00],\n",
      "        [  1.06478000e+03,   1.86658750e-01]],\n",
      "\n",
      "       [[  1.06498000e+03,   3.63000000e-01],\n",
      "        [  1.06498000e+03,   1.00000000e+00],\n",
      "        [  1.06498000e+03,   5.71372200e-02],\n",
      "        ..., \n",
      "        [  1.06599000e+03,   2.47721130e-01],\n",
      "        [  1.06599000e+03,   1.21927887e+00],\n",
      "        [  1.06599000e+03,   4.64721130e-01]]]), 'y': array([[ 1051.34,  1051.33,  1050.76,  1051.33,  1051.33,  1051.48,\n",
      "         1051.35,  1051.33,  1051.48,  1051.51,  1051.98,  1052.4 ,\n",
      "         1051.52,  1051.57,  1052.48,  1052.48,  1054.48,  1054.39,\n",
      "         1053.39,  1054.4 ,  1054.39,  1053.66,  1053.67,  1054.99,\n",
      "         1054.44,  1054.44,  1054.44,  1054.39,  1054.45,  1054.45,\n",
      "         1054.45,  1054.72],\n",
      "       [ 1058.54,  1059.1 ,  1059.12,  1059.12,  1057.74,  1057.74,\n",
      "         1058.54,  1058.54,  1057.73,  1058.54,  1059.1 ,  1059.1 ,\n",
      "         1057.74,  1058.53,  1059.1 ,  1059.75,  1057.74,  1055.78,\n",
      "         1058.25,  1058.53,  1059.73,  1057.76,  1059.39,  1059.74,\n",
      "         1059.74,  1059.74,  1059.98,  1060.  ,  1060.2 ,  1059.78,\n",
      "         1060.47,  1060.5 ],\n",
      "       [ 1064.98,  1064.98,  1064.98,  1064.95,  1064.97,  1064.95,\n",
      "         1064.95,  1064.97,  1064.95,  1064.95,  1064.62,  1064.97,\n",
      "         1064.98,  1064.62,  1064.62,  1064.97,  1064.97,  1064.98,\n",
      "         1064.96,  1064.96,  1064.96,  1064.98,  1064.98,  1064.98,\n",
      "         1064.96,  1064.99,  1064.99,  1064.99,  1064.99,  1064.99,\n",
      "         1064.99,  1064.99],\n",
      "       [ 1062.78,  1063.9 ,  1062.82,  1063.9 ,  1063.9 ,  1063.9 ,\n",
      "         1063.9 ,  1062.87,  1063.9 ,  1063.9 ,  1063.9 ,  1063.9 ,\n",
      "         1063.49,  1063.49,  1063.49,  1063.49,  1063.49,  1063.49,\n",
      "         1063.49,  1063.49,  1063.9 ,  1064.51,  1062.48,  1062.48,\n",
      "         1063.  ,  1062.47,  1062.  ,  1060.34,  1060.27,  1060.  ,\n",
      "         1060.88,  1062.6 ],\n",
      "       [ 1065.91,  1065.91,  1065.99,  1065.99,  1066.  ,  1065.99,\n",
      "         1065.99,  1065.84,  1065.84,  1065.84,  1065.84,  1065.82,\n",
      "         1065.79,  1065.76,  1065.76,  1065.76,  1066.  ,  1066.  ,\n",
      "         1066.  ,  1066.  ,  1066.  ,  1066.06,  1066.06,  1066.06,\n",
      "         1066.27,  1066.27,  1066.27,  1066.24,  1066.24,  1066.  ,\n",
      "         1065.  ,  1064.62]])}\n"
     ]
    }
   ],
   "source": [
    "rd = np.load('sample_data.npy')\n",
    "data = transform_to_seq(rd[:,[1,2]])\n",
    "print(batcher(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the model.\n",
    "def RNN_model():\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, NUM_STEPS, 2], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.float32, [None, NUM_STEPS], name='labels_placeholder')\n",
    "\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(NUM_STEPS)\n",
    "    \n",
    "    state = tf.zeros([BATCH_SIZE, lstm.state_size])\n",
    "    \n",
    "    outputs, states = tf.contrib.rnn.basic_rnn_seq2seq(x, y, lstm_cell, feed_previous=False)\n",
    "    \n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    \"\"\"\n",
    "\n",
    "    rnn_inputs = tf.one_hot(x, num_classes)\n",
    "\n",
    "    \"\"\"\n",
    "    RNN\n",
    "    \"\"\"\n",
    "    \n",
    "    cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "    \"\"\"\n",
    "    Predictions, loss, training step\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.reshape(\n",
    "                tf.matmul(tf.reshape(rnn_outputs, [-1, state_size]), W) + b,\n",
    "                [batch_size, num_steps, num_classes])\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    total_loss = tf.reduce_mean(losses)\n",
    "    train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a7d9cbe7a15e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRNN_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-3e81b2244876>\u001b[0m in \u001b[0;36mRNN_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlstm_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_rnn_seq2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_previous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lstm' is not defined"
     ]
    }
   ],
   "source": [
    "RNN_model()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:jupyter-tf]",
   "language": "python",
   "name": "conda-env-jupyter-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
